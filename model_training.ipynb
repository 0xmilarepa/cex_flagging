{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from models import *\n",
    "from main import *\n",
    "from data import *\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nativetx_df = pd.read_parquet('dataset/raw_data/transaction_native_seeder.parquet')\n",
    "tokentx_df = pd.read_parquet('dataset/raw_data/transaction_token_seeder.parquet')\n",
    "seed_df = pd.read_csv('dataset/raw_data/seed_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating training data\n",
    "\n",
    "### We'll begin by establishing the training data for our models by determining the total number of addresses that are present in seed_df which are actually active in the test sets, nativetx_df and tokentx_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total active addresses from seed_df in tokentx_df: 6708 out of 8619\n",
      "Total active addresses from seed_df in nativetx_df: 8101 out of 8619\n",
      "Total active CEX addresses in seed_df: 302 out of 8619\n"
     ]
    }
   ],
   "source": [
    "seed_addr = set(seed_df.eoa.unique())\n",
    "seed_native_addr = set(seed_df.loc[seed_df[\"eoa\"].isin(nativetx_df[\"FROM_ADDRESS\"]) | seed_df[\"eoa\"].isin(nativetx_df[\"TO_ADDRESS\"]) | seed_df[\"eoa\"].isin(nativetx_df[\"ORIGIN_TO_ADDRESS\"]) | seed_df[\"eoa\"].isin(nativetx_df[\"ORIGIN_FROM_ADDRESS\"]), \"eoa\"].unique())\n",
    "seed_token_addr = set(seed_df.loc[seed_df[\"eoa\"].isin(tokentx_df[\"FROM_ADDRESS\"]) | seed_df[\"eoa\"].isin(tokentx_df[\"TO_ADDRESS\"]) | seed_df[\"eoa\"].isin(tokentx_df[\"ORIGIN_TO_ADDRESS\"]) | seed_df[\"eoa\"].isin(tokentx_df[\"ORIGIN_FROM_ADDRESS\"]) | seed_df[\"eoa\"].isin(tokentx_df[\"CONTRACT_ADDRESS\"]), \"eoa\"].unique())\n",
    "cex_addresses = seed_df.loc[seed_df[\"prediction\"] == 1, \"eoa\"].tolist()\n",
    "print(f'Total active addresses from seed_df in tokentx_df: {len(seed_token_addr)} out of {len(seed_addr)}')\n",
    "print(f'Total active addresses from seed_df in nativetx_df: {len(seed_native_addr)} out of {len(seed_addr)}')\n",
    "print(f'Total active CEX addresses in seed_df: {len(cex_addresses)} out of {len(seed_addr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique addresses in nativetx_df: 60604\n",
      "Unique addresses in tokentx_df: 52719\n",
      "Total addresses from seed_df['eoa'] found in tokentx_df and nativetx_df: 8116.\n",
      "6693 unique addresses in common on both sets.\n"
     ]
    }
   ],
   "source": [
    "columns_to_check_tokentx = tokentx_df[[\"FROM_ADDRESS\", \"TO_ADDRESS\", \"ORIGIN_TO_ADDRESS\", \"ORIGIN_FROM_ADDRESS\", \"CONTRACT_ADDRESS\"]]\n",
    "columns_to_check_nativetx = nativetx_df[[\"FROM_ADDRESS\", \"TO_ADDRESS\", \"ORIGIN_TO_ADDRESS\", \"ORIGIN_FROM_ADDRESS\"]]\n",
    "\n",
    "native_addr = find_addresses_in_df(nativetx_df, columns_to_check_nativetx, cross_reference=False)\n",
    "token_addr = find_addresses_in_df(tokentx_df, columns_to_check_tokentx, cross_reference=False)\n",
    "active_tokentx_addr = find_addresses_in_df(tokentx_df, columns_to_check_tokentx, seed_addr)\n",
    "active_nativetx_addr = find_addresses_in_df(nativetx_df, columns_to_check_nativetx, seed_addr)\n",
    "active_addr = list(active_tokentx_addr.union(active_nativetx_addr))\n",
    "print(f'Unique addresses in nativetx_df: {len(native_addr)}')\n",
    "print(f'Unique addresses in tokentx_df: {len(token_addr)}')\n",
    "print(f\"Total addresses from seed_df['eoa'] found in tokentx_df and nativetx_df: {len(active_addr)}.\")\n",
    "print(f\"{len(active_nativetx_addr & active_tokentx_addr)} unique addresses in common on both sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 CEX addresses found in tokentx_df.\n",
      "204 CEX addresses found in nativetx_df.\n",
      "78 unique CEX addresses in common on both sets.\n"
     ]
    }
   ],
   "source": [
    "cex_in_tokentx = find_addresses_in_df(tokentx_df, columns_to_check_tokentx, set(cex_addresses))\n",
    "cex_in_nativetx = find_addresses_in_df(nativetx_df, columns_to_check_nativetx, set(cex_addresses))\n",
    "print(f\"{len(cex_in_tokentx)} CEX addresses found in tokentx_df.\")\n",
    "print(f\"{len(cex_in_nativetx)} CEX addresses found in nativetx_df.\")\n",
    "print(f\"{len(set(cex_in_nativetx) & set(cex_in_tokentx))} unique CEX addresses in common on both sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_active_df = seed_df[seed_df[\"eoa\"].isin(active_nativetx_addr)]\n",
    "seed_active_df = seed_active_df.rename(columns={\"eoa\": 'address'})\n",
    "seed_active_df = seed_active_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the cluster_sizes function, we'll establish a treshold to determine, in respect to a specific dataset, what's considered to be a \"large cluster\". We'll then adjust this number later to the pipeline preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nativetx_cluster_size = cluster_sizes(nativetx_df, active_nativetx_addr, resolution=1.5)\n",
    "# tokentx_cluster_size = cluster_sizes(tokentx_df, active_nativetx_addr, resolution=1.5)\n",
    "# print(f'nativetx_df cluster sizes: {nativetx_cluster_size.describe()}')\n",
    "# print('='*50)\n",
    "# print(f'tokentx_df cluster sizes: {tokentx_cluster_size.describe()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training our models\n",
    "\n",
    "### We've obtained a new DataFrame with only the active addresses and their predictions, which consists of 8,101 unique addresses. Since we previously established that nativetx_df contains the bulk of the activity of these addresses, we'll train our model based on the behavior they exhibit there. We'll process the data through our pipeline function, stored in preprocessing.py, to obtain a clean DataFrame with our engineered features, ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pipeline_processing(nativetx_df, active_nativetx_addr, timestamp_col=\"BLOCK_TIMESTAMP\", resolution= 1.5, min_cluster_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training our model, we'll add the 'prediction' column and double check that the preprocessing didn't shuffle the data associated with each address, hence altering the prediction tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.merge(\n",
    "#         seed_active_df[['address', 'prediction']],\n",
    "#         left_on='address',\n",
    "#         right_on='address',\n",
    "#         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = seed_active_df[['address', 'prediction']].merge(\n",
    "#     train_df[['address', 'prediction']],\n",
    "#     on='address',\n",
    "#     how='inner',\n",
    "#     suffixes=('_df1', '_df2')\n",
    "# )\n",
    "# merged_df['predictions_match'] = merged_df['prediction_df1'] == merged_df['prediction_df2']\n",
    "\n",
    "# mismatches = merged_df[~merged_df['predictions_match']]\n",
    "\n",
    "# print(f\"Total addresses: {len(merged_df)}\")\n",
    "# print(f\"Addresses with matching predictions: {len(merged_df[merged_df['predictions_match']])}\")\n",
    "# print(f\"Addresses with mismatched predictions: {len(mismatches)}\")\n",
    "\n",
    "# if not mismatches.empty:\n",
    "#     print(\"\\nMismatched predictions:\")\n",
    "#     print(mismatches)\n",
    "# else:\n",
    "#     print(\"\\nAwesome, all predictions match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our task of detecting CEX/Bridge addresses we'll train and compare three different models: a Random Forest Classifier, a Logistic Regression, and an Isolation Forest. We'll save our train_df to be able to call our main function and initialize, compile, train, evaluate and compare all three models, in order to see which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"dataset/processed_data/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"dataset/processed_data/train_data.csv\")\n",
    "X = ['outgoing_volume_USD', 'incoming_volume_USD', 'total_volume_USD', 'unique_interactions', 'tx_count', 'tx_per_hour', 'active_days', 'is_large_cluster', 'interaction_volume_USD']\n",
    "y = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outgoing_volume_USD</th>\n",
       "      <th>incoming_volume_USD</th>\n",
       "      <th>total_volume_USD</th>\n",
       "      <th>unique_interactions</th>\n",
       "      <th>tx_count</th>\n",
       "      <th>tx_per_hour</th>\n",
       "      <th>active_days</th>\n",
       "      <th>is_large_cluster</th>\n",
       "      <th>interaction_volume_USD</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8101.000000</td>\n",
       "      <td>8.101000e+03</td>\n",
       "      <td>8101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>157.140641</td>\n",
       "      <td>193.224182</td>\n",
       "      <td>350.364823</td>\n",
       "      <td>20.368350</td>\n",
       "      <td>55.363659</td>\n",
       "      <td>1.872795</td>\n",
       "      <td>251.850610</td>\n",
       "      <td>0.413776</td>\n",
       "      <td>1.022428e+04</td>\n",
       "      <td>0.025182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1098.738813</td>\n",
       "      <td>1082.746526</td>\n",
       "      <td>2082.866882</td>\n",
       "      <td>32.505507</td>\n",
       "      <td>126.366408</td>\n",
       "      <td>1.388280</td>\n",
       "      <td>186.641647</td>\n",
       "      <td>0.492540</td>\n",
       "      <td>1.403843e+05</td>\n",
       "      <td>0.156687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.444286</td>\n",
       "      <td>9.056667</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>72.517072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.788000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.481000</td>\n",
       "      <td>30.820000</td>\n",
       "      <td>55.420606</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.684932</td>\n",
       "      <td>242.789769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.319931e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>88.202000</td>\n",
       "      <td>120.053333</td>\n",
       "      <td>212.061667</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>2.035714</td>\n",
       "      <td>395.354803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.927859e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77217.274304</td>\n",
       "      <td>71435.731667</td>\n",
       "      <td>148653.005970</td>\n",
       "      <td>952.000000</td>\n",
       "      <td>5382.000000</td>\n",
       "      <td>100.800000</td>\n",
       "      <td>595.514769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.158001e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       outgoing_volume_USD  incoming_volume_USD  total_volume_USD  \\\n",
       "count          8101.000000          8101.000000       8101.000000   \n",
       "mean            157.140641           193.224182        350.364823   \n",
       "std            1098.738813          1082.746526       2082.866882   \n",
       "min               0.000000             0.000000          0.030000   \n",
       "25%               5.444286             9.056667         16.180000   \n",
       "50%              21.481000            30.820000         55.420606   \n",
       "75%              88.202000           120.053333        212.061667   \n",
       "max           77217.274304         71435.731667     148653.005970   \n",
       "\n",
       "       unique_interactions     tx_count  tx_per_hour  active_days  \\\n",
       "count          8101.000000  8101.000000  8101.000000  8101.000000   \n",
       "mean             20.368350    55.363659     1.872795   251.850610   \n",
       "std              32.505507   126.366408     1.388280   186.641647   \n",
       "min               0.000000     1.000000     1.000000     0.000174   \n",
       "25%               4.000000     6.000000     1.400000    72.517072   \n",
       "50%              11.000000    22.000000     1.684932   242.789769   \n",
       "75%              27.000000    63.000000     2.035714   395.354803   \n",
       "max             952.000000  5382.000000   100.800000   595.514769   \n",
       "\n",
       "       is_large_cluster  interaction_volume_USD   prediction  \n",
       "count       8101.000000            8.101000e+03  8101.000000  \n",
       "mean           0.413776            1.022428e+04     0.025182  \n",
       "std            0.492540            1.403843e+05     0.156687  \n",
       "min            0.000000            0.000000e+00     0.000000  \n",
       "25%            0.000000            9.788000e+01     0.000000  \n",
       "50%            0.000000            7.319931e+02     0.000000  \n",
       "75%            1.000000            3.927859e+03     0.000000  \n",
       "max            1.000000            8.158001e+06     1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTETomek: Counter({0: 5060, 1: 124})\n",
      "Class distribution after SMOTETomek: Counter({0: 5006, 1: 2476})\n",
      "\n",
      "Training random_forest ✅\n",
      "Best Parameters for RandomForestClassifier: {'model__max_depth': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n",
      "\n",
      "Evaluation metrics for Validation set:\n",
      "Accuracy: 0.9737654320987654\n",
      "F1-Score (Weighted): 0.9759854231741782\n",
      "F1-Score (Macro): 0.8265060240963855\n",
      "ROC-AUC Score: 0.9781746031746031\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1254\n",
      "           1       0.57      0.81      0.67        42\n",
      "\n",
      "    accuracy                           0.97      1296\n",
      "   macro avg       0.78      0.89      0.83      1296\n",
      "weighted avg       0.98      0.97      0.98      1296\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1228   26]\n",
      " [   8   34]]\n",
      "\n",
      "Evaluation metrics for Test set:\n",
      "Accuracy: 0.9710055521283159\n",
      "F1-Score (Weighted): 0.9744785868846179\n",
      "F1-Score (Macro): 0.7598450389451552\n",
      "ROC-AUC Score: 0.9640755394487484\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1583\n",
      "           1       0.43      0.71      0.53        38\n",
      "\n",
      "    accuracy                           0.97      1621\n",
      "   macro avg       0.71      0.84      0.76      1621\n",
      "weighted avg       0.98      0.97      0.97      1621\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1547   36]\n",
      " [  11   27]]\n",
      "\n",
      "Test set predictions for random_forest:\n",
      "                                         address  prediction  probability\n",
      "2818  0xc53539fd4d109840dbfe8c759ba7fc2e7200709b           0     0.296667\n",
      "6922  0xe9059c5e316a2d9f476fb03fe2af20a58a200b29           0     0.133333\n",
      "5993  0x7482c861aecfe16ff7c8c55ce4cde52d5225ee8c           0     0.000000\n",
      "3245  0x8380eaa4f63bfb981b413bad5016e971015a251a           0     0.003333\n",
      "7082  0xfda9c289eb901d495dec764050f9d596eb64cbbe           0     0.163333\n",
      "\n",
      "Unique addresses flagged as CEX by random_forest: 208\n",
      "\n",
      "First few flagged addresses for random_forest:\n",
      "                                      address  is_cex\n",
      "0  0x50b0aabf36b21e72add83b8904cb52bfe0171f66       0\n",
      "1  0x4e29fa717fb61753e26885421b84ff7e06df585e       1\n",
      "2  0x5507dbd48a5a5bace8a6030e878cc4e0af147c33       0\n",
      "3  0xd9185e233575f4e0d0e83159fdc6dfe9107bbf4d       0\n",
      "4  0xd2578c95c2daf87e7542d4c305c95cef01295877       0\n",
      "\n",
      "Training logistic_regression ✅\n",
      "Best Parameters for LogisticRegression: {'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
      "\n",
      "Evaluation metrics for Validation set:\n",
      "Accuracy: 0.8125\n",
      "F1-Score (Weighted): 0.8719336041185695\n",
      "F1-Score (Macro): 0.5679946666008222\n",
      "ROC-AUC Score: 0.9205209994683679\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.89      1254\n",
      "           1       0.14      0.93      0.24        42\n",
      "\n",
      "    accuracy                           0.81      1296\n",
      "   macro avg       0.57      0.87      0.57      1296\n",
      "weighted avg       0.97      0.81      0.87      1296\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1014  240]\n",
      " [   3   39]]\n",
      "\n",
      "Evaluation metrics for Test set:\n",
      "Accuracy: 0.8260333127698951\n",
      "F1-Score (Weighted): 0.8858449417183609\n",
      "F1-Score (Macro): 0.5461060934632891\n",
      "ROC-AUC Score: 0.9131562323370017\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.90      1583\n",
      "           1       0.11      0.87      0.19        38\n",
      "\n",
      "    accuracy                           0.83      1621\n",
      "   macro avg       0.55      0.85      0.55      1621\n",
      "weighted avg       0.98      0.83      0.89      1621\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1306  277]\n",
      " [   5   33]]\n",
      "\n",
      "Test set predictions for logistic_regression:\n",
      "                                         address  prediction  probability\n",
      "2818  0xc53539fd4d109840dbfe8c759ba7fc2e7200709b           0     0.000242\n",
      "6922  0xe9059c5e316a2d9f476fb03fe2af20a58a200b29           0     0.424434\n",
      "5993  0x7482c861aecfe16ff7c8c55ce4cde52d5225ee8c           0     0.000278\n",
      "3245  0x8380eaa4f63bfb981b413bad5016e971015a251a           0     0.001664\n",
      "7082  0xfda9c289eb901d495dec764050f9d596eb64cbbe           1     0.910802\n",
      "\n",
      "Unique addresses flagged as CEX by logistic_regression: 800\n",
      "\n",
      "First few flagged addresses for logistic_regression:\n",
      "                                      address  is_cex\n",
      "0  0x50b0aabf36b21e72add83b8904cb52bfe0171f66       0\n",
      "1  0x4e29fa717fb61753e26885421b84ff7e06df585e       1\n",
      "2  0x5507dbd48a5a5bace8a6030e878cc4e0af147c33       0\n",
      "3  0xd9185e233575f4e0d0e83159fdc6dfe9107bbf4d       0\n",
      "4  0xd2578c95c2daf87e7542d4c305c95cef01295877       0\n"
     ]
    }
   ],
   "source": [
    "sup_models = main_sup(train_df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly-based CEX Flags: 810\n",
      "Best Parameters: {'contamination': 0.1, 'max_features': 0.5, 'max_samples': 200, 'n_estimators': 200}\n",
      "ROC-AUC Score: 0.5191149778893449\n",
      "Confusion Matrix:\n",
      " [[7115  782]\n",
      " [ 176   28]]\n",
      "Precision: 0.0345679012345679\n",
      "Recall: 0.13725490196078433\n",
      "F1-Score: 0.055226824457593686\n",
      "Model saved to results/isolation_forest_model.pkl\n",
      "Results saved to results/isolation_forest_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "if_model = iso_forest(train_df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>is_cex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x50b0aabf36b21e72add83b8904cb52bfe0171f66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x4e29fa717fb61753e26885421b84ff7e06df585e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x5507dbd48a5a5bace8a6030e878cc4e0af147c33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xd9185e233575f4e0d0e83159fdc6dfe9107bbf4d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xd2578c95c2daf87e7542d4c305c95cef01295877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>0xb642d13f3bf889a4c789df387c35caa3e007207d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8097</th>\n",
       "      <td>0xe7064cc9eb9a44a67162db2b6275dfedb3c490c2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8098</th>\n",
       "      <td>0x1765585f0177b378b6819118a0f1a7822b141913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8099</th>\n",
       "      <td>0xbef2d9718f241c874b1ca09e4f757888ce95d57b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>0x97177a16bb141f6864c5e4682499d69c2a8114e9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         address  is_cex\n",
       "0     0x50b0aabf36b21e72add83b8904cb52bfe0171f66       0\n",
       "1     0x4e29fa717fb61753e26885421b84ff7e06df585e       0\n",
       "2     0x5507dbd48a5a5bace8a6030e878cc4e0af147c33       0\n",
       "3     0xd9185e233575f4e0d0e83159fdc6dfe9107bbf4d       0\n",
       "4     0xd2578c95c2daf87e7542d4c305c95cef01295877       0\n",
       "...                                          ...     ...\n",
       "8096  0xb642d13f3bf889a4c789df387c35caa3e007207d       0\n",
       "8097  0xe7064cc9eb9a44a67162db2b6275dfedb3c490c2       0\n",
       "8098  0x1765585f0177b378b6819118a0f1a7822b141913       0\n",
       "8099  0xbef2d9718f241c874b1ca09e4f757888ce95d57b       0\n",
       "8100  0x97177a16bb141f6864c5e4682499d69c2a8114e9       0\n",
       "\n",
       "[8101 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrectly flagged addresses: 782\n",
      "\n",
      "Incorrectly Flagged Addresses:\n",
      "                                         address  is_cex  prediction\n",
      "2     0x5507dbd48a5a5bace8a6030e878cc4e0af147c33       1           0\n",
      "4     0xd2578c95c2daf87e7542d4c305c95cef01295877       1           0\n",
      "5     0x81e877dd467f65b79aff559a8fafed6e95f01ad8       1           0\n",
      "9     0xf1e7dbe363dcb884b2d860e7dd7b3a675486e5ee       1           0\n",
      "14    0xd6ca66a00997a2038a6dc7304131a40f48268f77       1           0\n",
      "...                                          ...     ...         ...\n",
      "7895  0x5e76ff23db38fab6746d1026daa53ef9dde15614       1           0\n",
      "7921  0x0bc4b9b8e6551c848beae8fe1375b470f8a3b206       1           0\n",
      "7954  0xc96d84ec51362b7cf37c155dd842502a0b904dc6       1           0\n",
      "7956  0xde7a3869b325e8e609e8ccbee818cc27eedbf91a       1           0\n",
      "7958  0x1b107a7ff0be3c77f93543047d1e09437dbc7133       1           0\n",
      "\n",
      "[782 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on the address column (not the index)\n",
    "merged_df = pd.merge(if_model[0], train_df[['address', 'prediction']], on='address', how='left')\n",
    "\n",
    "# Identify incorrectly flagged addresses (is_cex = 1 but prediction = 0)\n",
    "incorrectly_flagged = merged_df[(merged_df['is_cex'] == 1) & (merged_df['prediction'] == 0)]\n",
    "\n",
    "# Create a DataFrame with the incorrectly flagged addresses\n",
    "incorrectly_flagged_df = incorrectly_flagged[['address', 'is_cex', 'prediction']]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of incorrectly flagged addresses: {len(incorrectly_flagged_df)}\")\n",
    "print(\"\\nIncorrectly Flagged Addresses:\")\n",
    "print(incorrectly_flagged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test data from nativetx_df and tokentx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    60604.00000\n",
       "mean       830.73444\n",
       "std       1129.62028\n",
       "min          0.00000\n",
       "25%         54.00000\n",
       "50%        487.00000\n",
       "75%        902.00000\n",
       "max       4444.00000\n",
       "Name: cluster_size, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nativetx_large_clusters = cluster_sizes(nativetx_df, native_addr, resolution= 1.5)\n",
    "nativetx_large_clusters.cluster_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52719.000000\n",
       "mean       625.719627\n",
       "std        809.488410\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%        100.000000\n",
       "75%       1091.000000\n",
       "max       2393.000000\n",
       "Name: cluster_size, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokentx_large_clusters = cluster_sizes(tokentx_df, token_addr, resolution= 1.5)\n",
    "tokentx_large_clusters.cluster_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume processing done ✅\n",
      "Unique interactions processing done ✅\n",
      "Activity processing done ✅\n",
      "Cluster processing done ✅\n",
      "All done ✅\n"
     ]
    }
   ],
   "source": [
    "# native_test = pipeline_processing(nativetx_df, native_addr, origin_from_col='ORIGIN_FROM_ADDRESS',\n",
    "#                                   origin_to_col='ORIGIN_TO_ADDRESS', timestamp_col=\"BLOCK_TIMESTAMP\",\n",
    "#                                   resolution= 1.5, min_cluster_size=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume processing done ✅\n",
      "Unique interactions processing done ✅\n",
      "Activity processing done ✅\n",
      "Cluster processing done ✅\n",
      "All done ✅\n"
     ]
    }
   ],
   "source": [
    "# token_test = pipeline_processing(tokentx_df, token_addr, origin_from_col='ORIGIN_FROM_ADDRESS',\n",
    "#                                   origin_to_col='ORIGIN_TO_ADDRESS', contract_address_col=\"CONTRACT_ADDRESS\",\n",
    "#                                   resolution= 1.5, min_cluster_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered shape: (46011, 10)\n",
      "Original shape: (52719, 10)\n"
     ]
    }
   ],
   "source": [
    "# native_test_data = native_test[~native_test['address'].isin(seed_df['eoa'])]\n",
    "# token_test_data = token_test[~token_test['address'].isin(seed_df['eoa'])]\n",
    "# print(\"Filtered shape:\", native_test_data.shape)\n",
    "# print(\"Original shape:\", native_test.shape)\n",
    "# print(\"Filtered shape:\", token_test_data.shape)\n",
    "# print(\"Original shape:\", token_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native_test_data.to_csv(\"dataset/processed_data/native_test_data.csv\", index=False)\n",
    "# token_test_data.to_csv(\"dataset/processed_data/token_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_test = pd.read_csv(\"dataset/processed_data/native_test_data.csv\")\n",
    "X_native_test = native_test.drop(columns=['address'])\n",
    "native_test_addr = native_test.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = pd.read_csv(\"dataset/processed_data/token_test_data.csv\")\n",
    "X_token_test = native_test.drop(columns=['address'])\n",
    "token_test_addr = token_test.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9783 addresses in common out of 98514 unique addresses in both sets\n"
     ]
    }
   ],
   "source": [
    "common_addresses = set(native_test[\"address\"]).intersection(set(token_test[\"address\"]))\n",
    "print(f\"{len(common_addresses)} addresses in common out of {len(native_test_addr) + len(token_test_addr)} unique addresses in both sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing our best model on test set. \n",
    "### We'll procede to test our best model, the already downloaded Random Forest, and use it to classify our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = load('results/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_y_proba = rf_best.predict_proba(X_native_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_results = prediction_df(native_y_proba, native_test_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_results.to_csv(\"predictions/native_dataset_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0xmilarepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
